<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Openshift on Underkube</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/tags/openshift/</link><description>Recent content in Openshift on Underkube</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 08 Mar 2021 08:30:00 +0000</lastBuildDate><atom:link href="https://e-minguez.github.io/underkube/pr-preview/pr-44/tags/openshift/index.xml" rel="self" type="application/rss+xml"/><item><title>Customizing OpenShift 4 baremetal IPI network at installation time</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/custom-network-openshift-ipi-install-time/</link><pubDate>Mon, 08 Mar 2021 08:30:00 +0000</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/custom-network-openshift-ipi-install-time/</guid><description>When deploying OpenShift IPI on baremetal, there is only so much you can tweak at installation time in terms of networking. Of course you can do changes after the installation, such as applying bonding configurations or vlan settings via machine configs&amp;hellip; but what if you need those changes at installation time?
In my case, I have an OpenShift environment composed by physical servers where each of them have 4 NICs. 1 unplugged NIC, 1 NIC connected to the provisioning network and 2 NICs connected to the same switch and to the same baremetal subnet.</description></item><item><title>Using an external registry with OpenShift 4</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/external-registry-ocp/</link><pubDate>Thu, 11 Feb 2021 08:30:00 +0000</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/external-registry-ocp/</guid><description>In this blog post I&amp;rsquo;m trying to perform the integration of an external registry with an OpenShift environment.
The external registry can be any container registry, but in this case I&amp;rsquo;ve configured harbor to use certificates (self generated), the &amp;rsquo;library&amp;rsquo; repository in the harbor registry to be private (aka. require user/pass) and created an &amp;rsquo;edu&amp;rsquo; user account with permissions on that &amp;rsquo;library&amp;rsquo; repository.
Harbor installation ðŸ”—Pretty straightforward if following the docs, but for RHEL7:</description></item><item><title>Deploy Inspektor Gadget on OpenShift 4.6</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/inspektor-gadget-openshift/</link><pubDate>Thu, 03 Dec 2020 08:30:00 +0000</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/inspektor-gadget-openshift/</guid><description>Introduction ðŸ”—Inspektor Gadget is a collection of tools (or gadgets) to debug and inspect Kubernetes applications.
Inspektor Gadget is deployed to each node as a privileged DaemonSet. It uses in-kernel BPF helper programs to monitor events mainly related to syscalls from userspace programs in a pod. The BPF programs are run by the kernel and gather the log data. Inspektor Gadget&amp;rsquo;s userspace utilities fetch the log data from ring buffers and display it.</description></item><item><title>Deploy OpenShift Virtualization 2.5 on OCP 4.6.1 on baremetal IPI</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/openshift-virtualization-on-ocp/</link><pubDate>Thu, 19 Nov 2020 09:30:42 +0200</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/openshift-virtualization-on-ocp/</guid><description>Preparation ðŸ”—Ensure your workers have the virtualization flag enabled:
for node in $(oc get nodes -o name | grep kni1-worker); do oc debug ${node} -- grep -c -E &amp;#39;vmx|svm&amp;#39; /host/proc/cpuinfo done That snippet should return the number of cpu cores with virtualization enabled (it should be all of them).
Subscription ðŸ”—cat &amp;lt;&amp;lt;EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: openshift-cnv --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: kubevirt-hyperconverged-group namespace: openshift-cnv spec: targetNamespaces: - openshift-cnv --- apiVersion: operators.</description></item><item><title>Deploy OCS 4.5 on OCP 4.6.1 on baremetal IPI</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/ocs-on-ocp/</link><pubDate>Wed, 04 Nov 2020 11:30:42 +0200</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/ocs-on-ocp/</guid><description>Preparation ðŸ”—Label the nodes you want to use for OCS, in my case:
for node in $(oc get nodes -o name | grep kni1-worker); do oc label ${node} cluster.ocs.openshift.io/openshift-storage=&amp;#39;&amp;#39; done Local storage operator ðŸ”—Deploy the local storage operator
cat &amp;lt;&amp;lt;EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: local-storage spec: {} --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: annotations: olm.providedAPIs: LocalVolume.v1.local.storage.openshift.io name: local-storage namespace: local-storage spec: targetNamespaces: - local-storage --- apiVersion: operators.</description></item><item><title>SCC assignments and permissions in OpenShift</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/custom-scc/</link><pubDate>Thu, 01 Oct 2020 10:18:42 +0200</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/custom-scc/</guid><description>SCCs ðŸ”—There are tons of information out there about SCCs, but in this post we will be focused on how to create and use a custom SCC only.
See the OpenShift official documentation on Managing Security Context Constraints for more details.
Custom SCC ðŸ”—In the event of requiring a custom SCC, there are a few steps that need to be done to be able to use the SCC properly.
Minimal capabilities ðŸ”—The best way to create a custom SCC would be to build it based on the most restricted one (hint: its name is restricted) and then start adding capabilities and permissions depending on the application requisites.</description></item><item><title>Simulate ONTAP Â® 9.6 on KVM + Trident 20.04 on OCP4</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/trident-ontap-ocp4/</link><pubDate>Mon, 11 May 2020 15:19:14 +0200</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/trident-ontap-ocp4/</guid><description>Introduction ðŸ”—NetApp filers can be used to provide dynamic storage to OCP4/k8s using NetApp&amp;rsquo;s Trident storage orchestrator.
In order to be able to use it, you need to have a real NetApp hardware to play with. It is also true that NetApp offers a simulator to play with.
NOTE: The Simulator is not publicly available and you can only access to it if you are a customer or partner. It is required for you to have a proper NFS license.</description></item><item><title>metallb on OCP4 baremetal</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/metallb-on-ocp4-baremetal/</link><pubDate>Fri, 05 Jul 2019 17:19:14 +0200</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/metallb-on-ocp4-baremetal/</guid><description>UPDATE: I submitted a PR to the MetalLB docs on how to deploy MetalLB on OpenShift 4 and it has been merged \o/ so hopefully it will be live soon.
ORIGINAL BLOG POST: &amp;ndash;8&amp;lt;&amp;ndash;
This blog post illustrates my steps to deploy metallb on OCP4 running on baremetal.
Environment ðŸ”—I have an OCP4 environment running in a Red Hat lab using 3 baremetal hosts as masters + workers deployed using openshift-metal3/dev-scripts</description></item><item><title>Metal3</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/metal3/</link><pubDate>Tue, 25 Jun 2019 17:19:14 +0200</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/metal3/</guid><description>In this blog post, I&amp;rsquo;m going to try to explain in my own words a high level overview of what Metal3 is, the motivation behind it and some concepts related to a &amp;lsquo;baremetal operator&amp;rsquo;.
Let&amp;rsquo;s have some definitions!
Custom Resource Definition ðŸ”—The k8s API provides some out-of-the-box objects such as pods, services, etc. There are a few methods of extending the k8s API (such as API extensions) but since a few releases back, the k8s API can be extended easily with custom resources definitions (CRDs).</description></item><item><title>OCP4 UPI baremetal pxeless with static ips</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/ocp4_upi_baremetal_pxeless_static_ips/</link><pubDate>Wed, 19 Jun 2019 15:16:45 +0200</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/ocp4_upi_baremetal_pxeless_static_ips/</guid><description>Do you want to deploy an OCP4 cluster without using PXE and using static IPs?
I&amp;rsquo;ve got you covered. See my unsupported step by step instructions on how to doing it, including:
No PXE (pretty common scenario in big companies) Avoid installing stuff and use containers instead (instead yum/dnf install httpd, haproxy,&amp;hellip; use containers) Use rootless containers if possible Use Fedora29/RHEL8 stuff (nmcli, firewalld, etc.) Enjoy!</description></item><item><title>Run your own Ghost blog on OpenShift</title><link>https://e-minguez.github.io/underkube/pr-preview/pr-44/run-your-own-ghost-blog-on-openshift/</link><pubDate>Tue, 22 Jul 2014 20:32:25 +0000</pubDate><guid>https://e-minguez.github.io/underkube/pr-preview/pr-44/run-your-own-ghost-blog-on-openshift/</guid><description>Easy peasy:
Create a free OpenShift account Setup your environment Run the following command: rhc app create ghost nodejs-0.10 --env NODE_ENV=production --from-code https://github.com/openshift-quickstart/openshift-ghost-quickstart.git Profit Check this quickstart for more information, and the awesome OpenShift documentation</description></item></channel></rss>